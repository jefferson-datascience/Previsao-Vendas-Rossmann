{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Pacotes e Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação de Dados\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_fato = pd.read_csv('../BaseDados/DadosTreino.csv', delimiter=',', low_memory=False)\n",
    "df1_dimensao = pd.read_csv('../BaseDados/DadosLoja.csv', delimiter=',', low_memory=False)\n",
    "df1 = df1_fato.merge(df1_dimensao, how='left', on='Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Seleção dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o nosso objetivo é realizar a previsão dos próximos 6 dias de vendas das lojas, podemos, após um entendimento das variáveis, verificar quais delas fazem sentido. \n",
    "\n",
    "1. Ao analisar, temos que duas variávies não fazem sentido para nós. São elas: 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval'.\n",
    "2. Obtendo apenas os dados dos últimos 7 dias de vendas.\n",
    "\n",
    "Portanto, vamos remover essas variáveis do nosso dataset e aplicar os filtros, de modo que, iniciemos nosso trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['Open'] == 1].drop(columns=['Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Tratamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste da Variável Data\n",
    "df2['Date'] = pd.to_datetime(df2['Date'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Eliminando os dados em que o competition distance é nulo\n",
    "df2 = df2.drop(index=df2[df2['CompetitionDistance'].isna()].sort_values(by=['Store', 'Date'], ascending=True).index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Dados Nulos: \n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Obtendo a menor data de vendas em relação a cada loja.\n",
    "competition_date = df2[['Store', 'Date']].groupby('Store').min().reset_index()\n",
    "\n",
    "# Vamos extrair o anoa dessas datas\n",
    "competition_date['MinDate'] =  competition_date['Date'].apply(lambda x: x.year)\n",
    "\n",
    "# Eliminando colunas desnecessárias\n",
    "competition_date = competition_date.drop(columns=['Date'], axis=1)\n",
    "\n",
    "# Aplicação de um left join para preencher os dados nulos\n",
    "df2 = df2.merge(competition_date, how='left', on='Store')\n",
    "\n",
    "# Eliminando os Dados Nulos dos Anos\n",
    "df2['CompetitionOpenSinceYear'] = np.where(df2['CompetitionOpenSinceYear'].isna(), df2['MinDate'], df2['CompetitionOpenSinceYear'])\n",
    "\n",
    "# Validação\n",
    "print(f'Quantidade de Dados Nulos: \\n{df2.isna().sum()[df2.isna().sum() > 0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Dados Nulos: \n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Obtendo a menor data de vendas em relação a cada loja.\n",
    "competition_date = df2[['Store', 'Date']].groupby('Store').min().reset_index()\n",
    "\n",
    "# Vamos extrair o anoa dessas datas\n",
    "competition_date['MinMonth'] =  competition_date['Date'].apply(lambda x: x.month)\n",
    "\n",
    "# Eliminando colunas desnecessárias\n",
    "competition_date = competition_date.drop(columns=['Date'], axis=1)\n",
    "\n",
    "# Aplicação de um left join para preencher os dados nulos\n",
    "df2 = df2.merge(competition_date, how='left', on='Store')\n",
    "\n",
    "# Eliminando os Dados Nulos dos Anos\n",
    "df2['CompetitionOpenSinceMonth'] = np.where(df2['CompetitionOpenSinceMonth'].isna(), df2['MinDate'], df2['CompetitionOpenSinceMonth'])\n",
    "\n",
    "# Validação\n",
    "print(f'Quantidade de Dados Nulos: \\n{df2.isna().sum()[df2.isna().sum() > 0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropando as colunas desnecessárias\n",
    "df2 = df2.drop(columns=['MinMonth', 'MinDate'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Engenharia de Atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo dessa etapa é conseguir ver a possibilidade de informações que estão 'escondidas' nos dados. Podemos obervar que o nosso dataset é bem limitado, sendo assim, vamos inicialmente construir nossa variável que é as vendas dos próximos 6 dias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cópia dos Dados\n",
    "df3 = df2.copy()\n",
    "\n",
    "# Ordenamento dos Dados\n",
    "df3_sort = df3.sort_values(by=['Store', 'Date'], ascending=True).reset_index(drop=True).copy()\n",
    "\n",
    "max_data = df3_sort.groupby('Store')['Date'].transform('max')\n",
    "df3_sort = df3_sort[df3_sort['Date'] >= (max_data - pd.Timedelta(days=7))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construção da variável public holiday\n",
    "dummies_producao = pd.get_dummies(data=df2_sort['StateHoliday'], prefix='StateHoliday', dtype=np.int64)\n",
    "\n",
    "if 'StateHoliday_a' in dummies_producao:\n",
    "    df3_sort['PublicHoliday'] = dummies_producao['StateHoliday_a']\n",
    "else:\n",
    "    df3_sort['PublicHoliday'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vendas do último dia\n",
    "df3_sort['SalesLastDay'] = df3_sort.groupby('Store')['Sales'].shift(1)\n",
    "\n",
    "# Média dos últimos 7 dias\n",
    "df3_sort['MeanSalesLastSevenDays'] = df3_sort.groupby('Store')['Sales'].shift(1).rolling(7).mean()\n",
    "\n",
    "# Total de vendas dos últimos 7 dias\n",
    "df3_sort['TotalSalesLastSevenDays'] = df3_sort.groupby('Store')['Sales'].shift(1).rolling(7).sum()\n",
    "\n",
    "# Total de Promoções aplicadas nos últimos 7 dias\n",
    "df3_sort['TotalPromoLastSevenDays'] = df3_sort.groupby('Store')['Promo'].shift(1).rolling(7).sum()\n",
    "\n",
    "# Total de Feriados Públicos nos últimos 7 dias\n",
    "df3_sort['TotalPublicHolidaysLastSevenDays'] = df3_sort.groupby('Store')['PublicHoliday'].shift(1).rolling(7).sum()\n",
    "\n",
    "# Total de promoções 2 ativadas nos últimos 7 dias\n",
    "df3_sort['TotalPromo2LastSevenDays'] = df3_sort.groupby('Store')['Promo2'].shift(1).rolling(7).sum()\n",
    "\n",
    "# Total de feriados escolares no últimos 7 dias\n",
    "df3_sort['TotalSchoolHolidayLastSevenDays'] = df3_sort.groupby('Store')['SchoolHoliday'].shift(1).rolling(7).sum()\n",
    "\n",
    "# Anos de Competição\n",
    "df3_sort['YearsCompetition'] = df3_sort['Date'].max().year - df3_sort['CompetitionOpenSinceYear']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vamos selecionar apenas as variáveis que façam sentido para o nosso problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_sort = df3_sort[['Store', 'StoreType', 'Assortment', 'CompetitionDistance', 'YearsCompetition', 'Sales', 'Customers', 'Promo','Promo2', 'PublicHoliday', 'SalesLastDay',\n",
    "                     'MeanSalesLastSevenDays', 'TotalSalesLastSevenDays', 'TotalPromoLastSevenDays', 'TotalPromo2LastSevenDays', 'TotalSchoolHolidayLastSevenDays', 'TotalPublicHolidaysLastSevenDays']]\n",
    "\n",
    "# Renomeando colunas\n",
    "df3_sort = df3_sort.rename(columns={'Sales':'SalesDay', 'Customers': 'CustomersDay', 'PromoDay':'Promo2Day', 'PublicHoliday':'PublicHolidayDay'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que a nossa variável alvo nossa é a TotalSalesNextSixDays. Sendo assim, nossa variável alvo não pode ser nula. Além disso, de modo que tenhamos o máximo de informação consolidada, vamos eliminar a quantidade as variáveis nulas em relação a soma móveis, pois pelo fato de ser nulo, quer dizer que não há informações disponíveis, logo, não está trazendo o conteúdo que precisamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminando os dados desnecessários\n",
    "df3_sort_clean = df3_sort.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Previsão das Próximos 6 dias de Faturamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cópia segura dos dados\n",
    "df4 = df3_sort_clean.drop(columns=['Store'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o modelo\n",
    "modelo_preditivo = pkl.load(open('../Modelos/XGBRegressorFinalVersao1.pkl', 'rb'))\n",
    "\n",
    "# Realizando a Predição\n",
    "predicao = modelo_preditivo.predict(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Construção da Base Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df3_sort_clean.copy()\n",
    "\n",
    "# Armazenando os dados no Dataset\n",
    "df5.loc[:,'PredictionSalesNextSexWeek'] = np.round(predicao.tolist(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Salvando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.to_parquet(path='../DadosPredicao/BasePredita.parquet', engine='fastparquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
